{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.29763387] 0.0 [-3.32360958]\n",
      "[1.297582219618919, -3.323609568074015, 10] 0.0\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "def f(x):\n",
    "    return x**3 * np.log(x) - 3*x\n",
    "\n",
    "def df(x):\n",
    "    return 3*x**2 * np.log(x) + x**3 * (1/x) - 3\n",
    "\n",
    "def d2f(x):\n",
    "    return 6*x*np.log(x) + 4\n",
    "\n",
    "def newtonsMethod(f,df,d2f,x,tol,alpha,beta):\n",
    "    i = 0\n",
    "    decrement = tol + 1   \n",
    "    while decrement > tol:      \n",
    "        hess = d2f(x)\n",
    "        grad = df(x)       \n",
    "        descentDir = - grad/hess    \n",
    "        stepsize = 1        \n",
    "        while f(x+stepsize*descentDir) > f(x)+alpha*stepsize*np.dot(descentDir,descentDir):         \n",
    "            stepsize = beta*stepsize\n",
    "        x = x + stepsize*descentDir\n",
    "        i = i+1\n",
    "        decrement = np.dot(grad,-descentDir)  \n",
    "    return [x,f(x),i]\n",
    "\n",
    "# Initial guess\n",
    "x0 = 1\n",
    "tol = 1e-6\n",
    "\n",
    "start_time = time.time()\n",
    "root_fsolve = fsolve(df, x0)\n",
    "time_fsolve = time.time() - start_time\n",
    "print(root_fsolve, time_fsolve, f(root_fsolve))\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "root_newton = newtonsMethod(f, df, d2f, x0, tol, 0.01, 0.8)\n",
    "time_newton = time.time() - start_time\n",
    "print(root_newton, time_newton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([nan, inf]), nan, 2] 0.0\n",
      "[array([-1.,  1.]), 2.0, 1] 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xinyi\\AppData\\Local\\Temp\\ipykernel_24404\\3302959486.py:3: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return -(-x[0]**2 / x[1] + x[0] * np.log(-x[0]) - x[1]**2)\n",
      "C:\\Users\\xinyi\\AppData\\Local\\Temp\\ipykernel_24404\\3302959486.py:3: RuntimeWarning: invalid value encountered in log\n",
      "  return -(-x[0]**2 / x[1] + x[0] * np.log(-x[0]) - x[1]**2)\n",
      "C:\\Users\\xinyi\\AppData\\Local\\Temp\\ipykernel_24404\\3302959486.py:7: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  dg_dx = -(-2*x[0]/x[1] + np.log(-x[0]) + 1)\n",
      "C:\\Users\\xinyi\\AppData\\Local\\Temp\\ipykernel_24404\\3302959486.py:7: RuntimeWarning: invalid value encountered in log\n",
      "  dg_dx = -(-2*x[0]/x[1] + np.log(-x[0]) + 1)\n",
      "C:\\Users\\xinyi\\AppData\\Local\\Temp\\ipykernel_24404\\3302959486.py:8: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  dg_dy = -(x[0]**2 / x[1]**2 - 2*x[1])\n"
     ]
    }
   ],
   "source": [
    "# Define the function to be maximized (minimize its negative)\n",
    "def g(x):\n",
    "    return -(-x[0]**2 / x[1] + x[0] * np.log(-x[0]) - x[1]**2)\n",
    "\n",
    "# Gradient of the function\n",
    "def grad_g(x):\n",
    "    dg_dx = -(-2*x[0]/x[1] + np.log(-x[0]) + 1)\n",
    "    dg_dy = -(x[0]**2 / x[1]**2 - 2*x[1])\n",
    "    return np.array([dg_dx, dg_dy])\n",
    "\n",
    "# Hessian matrix of the function\n",
    "def hess_g(x):\n",
    "    d2g_dxdx = -(2/x[1] + 1/x[0])\n",
    "    d2g_dydy = -(-x[0]**2 / x[1]**3 - 2)\n",
    "    d2g_dxdy = -(2*x[0] / x[1]**2)\n",
    "    return np.array([[d2g_dxdx, d2g_dxdy], [d2g_dxdy, d2g_dydy]])\n",
    "\n",
    "\n",
    "def gradDescentBacktracking(f,df,x,tol,alpha,beta):\n",
    "    descentDir = tol+1\n",
    "    i = 0   \n",
    "    while np.linalg.norm(descentDir) > tol:\n",
    "        i = i+1\n",
    "        descentDir = -df(x)\n",
    "        stepsize = 1        \n",
    "        while f(x+stepsize*descentDir) > f(x)+alpha*stepsize*np.dot(descentDir,descentDir):            \n",
    "            stepsize = beta*stepsize      \n",
    "        x = x + stepsize*descentDir        \n",
    "    return [x,g(x),i]\n",
    "\n",
    "def newtons_Method(f,df,d2f,x,tol,alpha,beta):\n",
    "    i = 0\n",
    "    decrement = tol + 1    \n",
    "    while decrement > tol:        \n",
    "        hess = d2f(x)\n",
    "        grad = df(x)        \n",
    "        descentDir = np.linalg.solve(-hess,grad)        \n",
    "        stepsize = 1        \n",
    "        while f(x+stepsize*descentDir) > f(x)+alpha*stepsize*np.dot(descentDir,descentDir):\n",
    "            \n",
    "            stepsize = beta*stepsize\n",
    "        x = x + stepsize*descentDir\n",
    "        i = i+1\n",
    "        decrement = np.dot(grad,-descentDir)       \n",
    "    return [x,g(x),i]\n",
    "\n",
    "# Initial guess\n",
    "x0 = np.array([-1, 1]) # Starting with x < 0 and y > 0\n",
    "tol = 1e-6\n",
    "\n",
    "start_time = time.time()\n",
    "root_back = gradDescentBacktracking(g, grad_g, x0, tol, 0.01, 0.8)\n",
    "time_back = time.time() - start_time\n",
    "print(root_back, time_back)\n",
    "\n",
    "start_time = time.time()\n",
    "root_newton = newtons_Method(g, grad_g, hess_g, x0, tol, 0.01, 0.8)\n",
    "time_newton = time.time() - start_time\n",
    "print(root_newton, time_newton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70248954] 0.0 [-0.68996106]\n",
      "[array([0.70248954]), array([-0.68996106]), 2] 0.0\n"
     ]
    }
   ],
   "source": [
    "def h(x):\n",
    "    return x**2 * (x**4 - 0.75*x**3 + 1.5*x**2 + 1.25*x - 3)\n",
    "\n",
    "# Define the first derivative of the function\n",
    "def dh(x):\n",
    "    return 2*x*(x**4 - 0.75*x**3 + 1.5*x**2 + 1.25*x - 3) + x**2 * (5*x**3 - 2.25*x**2 + 3*x + 1.25)\n",
    "\n",
    "# Define the second derivative of the function\n",
    "def d2h(x):\n",
    "    return 2*(x**4 - 0.75*x**3 + 1.5*x**2 + 1.25*x - 3) + 4*x*(5*x**3 - 2.25*x**2 + 3*x + 1.25) + x**2 * (15*x**2 - 4.5*x + 3)\n",
    "\n",
    "def gradDescentExact(df,x,tol):\n",
    "    i = 0\n",
    "    descentDir = tol + 1\n",
    "    while np.linalg.norm(descentDir) > tol:\n",
    "        i = i+1\n",
    "        descentDir = -df(x)\n",
    "        newX = lambda t: x + descentDir*t\n",
    "        dfdt = lambda t: np.dot(df(newX(t)),descentDir)\n",
    "        stepsize = fsolve(dfdt,0)\n",
    "        x = x + stepsize*descentDir\n",
    "    return [x,h(x),i]\n",
    "\n",
    "# Initial guess\n",
    "x0 = 1\n",
    "tol = 1e-6\n",
    "\n",
    "start_time = time.time()\n",
    "root_fsolve = fsolve(dh, x0)\n",
    "time_fsolve = time.time() - start_time\n",
    "print(root_fsolve, time_fsolve, h(root_fsolve))\n",
    "\n",
    "start_time = time.time()\n",
    "root_exact = gradDescentExact(dh, x0, tol)\n",
    "time_exact = time.time() - start_time\n",
    "print(root_exact, time_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-455.,   nan,    9.]), nan, 2] 0.0\n",
      "[array([ 1.41421356,  1.        , -1.        ]), 1.8466444930318665e-24, 6] 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xinyi\\AppData\\Local\\Temp\\ipykernel_24404\\1603100930.py:3: RuntimeWarning: invalid value encountered in log\n",
      "  return (x[0]**2 - 2)**2 + (np.log(x[1]))**2 + (x[1] + x[2])**2\n",
      "C:\\Users\\xinyi\\AppData\\Local\\Temp\\ipykernel_24404\\1603100930.py:9: RuntimeWarning: invalid value encountered in log\n",
      "  2 * (np.log(x[1])) / x[1] + 2 * (x[1] + x[2]),\n"
     ]
    }
   ],
   "source": [
    "# Define the function \n",
    "def l(x):\n",
    "    return (x[0]**2 - 2)**2 + (np.log(x[1]))**2 + (x[1] + x[2])**2\n",
    "\n",
    "# Define the gradient\n",
    "def dl(x):\n",
    "    return np.array([\n",
    "        4 * x[0] * (x[0]**2 - 2),\n",
    "        2 * (np.log(x[1])) / x[1] + 2 * (x[1] + x[2]),\n",
    "        2 * (x[1] + x[2])\n",
    "    ])\n",
    "\n",
    "# Define the Hessian\n",
    "def d2l(x):\n",
    "    return np.array([\n",
    "        [12 * x[0]**2 - 8, 0, 0],\n",
    "        [0, -2 / x[1]**2 + 2, 2],\n",
    "        [0, 2, 2]\n",
    "    ])\n",
    "    \n",
    "def gradDescentBacktracking(f,df,x,tol,alpha,beta):\n",
    "    descentDir = tol+1\n",
    "    i = 0    \n",
    "    while np.linalg.norm(descentDir) > tol:\n",
    "        i = i+1\n",
    "        descentDir = -df(x)\n",
    "        stepsize = 1        \n",
    "        while f(x+stepsize*descentDir) > f(x)+alpha*stepsize*np.dot(descentDir,descentDir):            \n",
    "            stepsize = beta*stepsize\n",
    "        x = x + stepsize*descentDir\n",
    "    return [x,l(x),i]\n",
    "\n",
    "def newtonsMethod(f,df,d2f,x,tol,alpha,beta):\n",
    "    i = 0\n",
    "    decrement = tol + 1\n",
    "    while decrement > tol:\n",
    "        hess = d2f(x)\n",
    "        grad = df(x)\n",
    "        descentDir = np.linalg.solve(-hess,grad)\n",
    "        stepsize = 1       \n",
    "        while f(x+stepsize*descentDir) > f(x)+alpha*stepsize*np.dot(descentDir,descentDir):          \n",
    "            stepsize = beta*stepsize      \n",
    "        x = x + stepsize*descentDir\n",
    "        i = i+1\n",
    "        decrement = np.dot(grad,-descentDir)  \n",
    "    return [x,l(x),i]\n",
    "\n",
    "# initial guess\n",
    "x0 = np.array([1, 1, 1])\n",
    "tol = 1e-6\n",
    "\n",
    "start_time = time.time()\n",
    "root_back = gradDescentBacktracking(l, dl, x0, tol, 0.01, 0.8)\n",
    "time_back = time.time() - start_time\n",
    "print(root_back, time_back)\n",
    "\n",
    "start_time = time.time()\n",
    "root_newton = newtonsMethod(l, dl, d2l, x0, tol, 0.01, 0.8)\n",
    "time_newton = time.time() - start_time\n",
    "print(root_newton, time_newton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.976630785306518\n",
      "[-0.07180806  0.48719137  0.25142353 -0.06523815 -0.12548226  0.52391357]\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Expected annual rate of return for each asset\n",
    "mu = np.array([0.2083, 0.1874, 0.2119, 0.1095, 0.0239, 0.001])\n",
    "\n",
    "# Covariance matrix for the assets\n",
    "Sigma = np.array([\n",
    "    [106.991461, 23.892217, -27.843578, -59.063559, 6.484768, 0.007298],\n",
    "    [23.892217, 98.632733, -117.56474, 226.16329, 16.106375, -0.006264],\n",
    "    [-27.843578, -117.56474, 206.012325, -40.45979, -6.732048, -0.006654],\n",
    "    [-59.063559, 226.16329, -40.45979, 1610.96212, -6.718355, -0.000654],\n",
    "    [6.484768, 16.106375, -6.732048, -6.718355, 1.234519, -0.0009],\n",
    "    [0.007298, -0.006264, -0.006654, -0.000654, -0.0009, 0.000013]\n",
    "])\n",
    "\n",
    "# Define the optimization variables\n",
    "x = cp.Variable(6)\n",
    "\n",
    "# The objective is to minimize the portfolio risk\n",
    "risk = cp.norm(x.T @ Sigma, 1)\n",
    "obj = cp.Minimize(risk)\n",
    "\n",
    "# The constraints are that the sum of x is 1 and the expected return is exactly 12%\n",
    "constraints = [cp.sum(x) == 1, \n",
    "               mu.T @ x == 0.12]\n",
    "\n",
    "# Formulate the optimization problem and solve it\n",
    "problem = cp.Problem(obj, constraints)\n",
    "problem.solve()\n",
    "    \n",
    "# Output the solution\n",
    "print(problem.value)\n",
    "print(x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal processor speeds: [0.79621003 0.79621003 0.79621003 0.79621003 0.79621003 0.79621003\n",
      " 0.79621003 0.79621003 0.79621003 0.79621003 0.79621003 0.79621003\n",
      " 0.79621003 0.79621003 0.79621003 0.79621003 0.79621003 0.79621003\n",
      " 0.79621003 0.79621003 0.79621003 0.79621003 0.79621003 0.79621003\n",
      " 0.79621003]\n",
      "\n",
      " Work allocation matrix S_it:\n",
      " [[0.1  0.1  0.1  0.1  0.1  0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.6  0.6  0.6  0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25\n",
      "  0.25 0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.8  0.8  0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25\n",
      "  0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.3  0.3  0.3  0.3  0.3  0.3  0.3\n",
      "  0.3  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.6  0.6  0.6  0.6  0.6  0.6  0.6\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.1  0.1  0.1  0.1  0.1  0.1\n",
      "  0.1  0.1  0.1  0.1  0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.2  0.2  0.2  0.2\n",
      "  0.2  0.2  0.2  0.2  0.2  0.2  0.2  0.2  0.2  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.9  0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.3  0.3  0.3 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Objective function with barrier\n",
    "def objective(s, mu):\n",
    "    energy = np.sum(s**2)\n",
    "    barrier_penalty = -mu * np.sum(np.log(s - s_min) + np.log(s_max - s))\n",
    "    return energy + barrier_penalty\n",
    "\n",
    "# Gradient of the objective function\n",
    "def gradient(s, mu):\n",
    "    grad_energy = 2 * s\n",
    "    grad_barrier = -mu * ((1 / (s - s_min)) - (1 / (s_max - s)))\n",
    "    return grad_energy + grad_barrier\n",
    "\n",
    "def optimize_processor_scheduling(s_min, s_max, T, n_jobs, W, A, D, alpha, tolerance):\n",
    "    \"\"\"\n",
    "    Optimize processor scheduling using a simplified barrier method.\n",
    "\n",
    "    Parameters:\n",
    "    s_min (float): Minimum processor speed.\n",
    "    s_max (float): Maximum processor speed.\n",
    "    T (int): Total number of time periods.\n",
    "    n_jobs (int): Total number of jobs.\n",
    "    W (numpy.array): Work required for each job.\n",
    "    A (numpy.array): Available time for each job.\n",
    "    D (numpy.array): Deadline for each job.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Optimized processor speeds and work allocation matrix.\n",
    "    \"\"\"\n",
    "    # Initialize processor speeds and work allocation\n",
    "    s = np.full(T, (s_min + s_max) / 2)\n",
    "    S_it = np.zeros((n_jobs, T))\n",
    "    \n",
    "    # Populate initial work allocation\n",
    "    for i in range(n_jobs):\n",
    "        work_per_period = W[i] / (D[i] - A[i])\n",
    "        S_it[i, A[i]:D[i]] = work_per_period\n",
    "\n",
    "    # Barrier parameter\n",
    "    mu = 1.0\n",
    "\n",
    "    # Optimization loop\n",
    "    while mu > tolerance:\n",
    "        # Gradient of the objective function\n",
    "        grad = gradient(s, mu)\n",
    "\n",
    "        # Gradient update step\n",
    "        s_new = s - alpha * grad\n",
    "        s_new = np.clip(s_new, s_min, s_max)\n",
    "\n",
    "        # Update work allocation matrix\n",
    "        for i in range(n_jobs):\n",
    "            time_indices = np.arange(A[i], D[i])\n",
    "            total_work = np.sum(s_new[time_indices])\n",
    "            work_distribution = s_new[time_indices] / total_work * W[i]\n",
    "            S_it[i, time_indices] = work_distribution\n",
    "\n",
    "        # Reduce barrier parameter and step size\n",
    "        mu *= 0.1\n",
    "        alpha *= 0.9\n",
    "\n",
    "        # Update processor speeds\n",
    "        s = s_new\n",
    "\n",
    "        # Convergence check\n",
    "        if np.linalg.norm(grad) * alpha < tolerance:\n",
    "            break\n",
    "\n",
    "    return s, S_it\n",
    "\n",
    "# Given data\n",
    "s_min = 0.25\n",
    "s_max = 1.5\n",
    "T = 25  # Total time periods\n",
    "n_jobs = 11  # Total number of jobs\n",
    "\n",
    "W = np.array([0.5, 1.8, 3, 1.6, 4.5, 2.4, 4.2, 1, 2.6, 0.9, 0.9]) # Work required for each job \n",
    "A = np.array([0, 4, 4, 5, 6, 7, 7, 8, 10, 15, 22]) # Time available for each job \n",
    "D = np.array([5, 7, 16, 7, 24, 15, 14, 18, 23, 16, 25]) # Deadline for each job \n",
    "\n",
    "optimal_speeds, optimal_work_allocation = optimize_processor_scheduling(s_min, s_max, T, n_jobs, W, A, D, 0.01, 1e-5)\n",
    "print(\"Optimal processor speeds:\", optimal_speeds)\n",
    "print(\"\\n Work allocation matrix S_it:\\n\", optimal_work_allocation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
