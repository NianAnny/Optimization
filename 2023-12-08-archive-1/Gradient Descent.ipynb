{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as sp\n",
    "\n",
    "# function to optimize\n",
    "f = lambda x: (x[0]-2)**2 + (x[1]-1)**4\n",
    "\n",
    "# its gradient\n",
    "gradf = lambda x: np.array([2*(x[0]-2),\n",
    "                           4*(x[1]-1)**3])\n",
    "\n",
    "# tolerance level\n",
    "tol = 1e-8\n",
    "\n",
    "# initial guess\n",
    "x0 = np.array([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# suppose we are at some x and we want to do one step of our pseudocode\n",
    "\n",
    "x = x0\n",
    "\n",
    "# evaluate gradient\n",
    "currentGrad = gradf(x)\n",
    "\n",
    "# choose a descent direction (ex: the negative gradient)\n",
    "deltax = -currentGrad\n",
    "\n",
    "# choose a stepsize (exact search method)\n",
    "newX = lambda t: x + t*deltax # new point as a function of t\n",
    "dfdt = lambda t: gradf(newX(t)) @ deltax # gradient of f wrt t\n",
    "\n",
    "# choose t such that df/dt = 0 (which minimizes the 1D problem)\n",
    "stepsize = sp.fsolve(dfdt, # function to find the root of\n",
    "                    0) # initial guess\n",
    "\n",
    "x = x + stepsize*deltax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now let's use the above code as the basis to write the full looped exact search algorithm\n",
    "\n",
    "def gradientDescentExact(f,gradf,x0,tol):\n",
    "    \n",
    "    # set initial x value\n",
    "    x = x0\n",
    "    \n",
    "    # set tolerance so we don't skip the while loop\n",
    "    mag = tol + 1\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    # while loop\n",
    "    while mag > tol:\n",
    "        \n",
    "        # evaluate gradient\n",
    "        currentGrad = gradf(x)\n",
    "\n",
    "        # choose a descent direction (ex: the negative gradient)\n",
    "        deltax = -currentGrad\n",
    "\n",
    "        # choose a stepsize (exact search method)\n",
    "        newX = lambda t: x + t*deltax # new point as a function of t\n",
    "        dfdt = lambda t: gradf(newX(t)) @ deltax # gradient of f wrt t\n",
    "\n",
    "        # choose t such that df/dt = 0 (which minimizes the 1D problem)\n",
    "        stepsize = sp.fsolve(dfdt, # function to find the root of\n",
    "                            0) # initial guess\n",
    "\n",
    "        x = x + stepsize*deltax\n",
    "        \n",
    "        # check stopping criterion\n",
    "        mag = currentGrad @ currentGrad\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "    return [x,f(x),counter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.99996499, 1.02596502]), 4.5574764160614557e-07, 183]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientDescentExact(f,gradf,x0,1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now let's use the modify this for the backtracking algorithm\n",
    "\n",
    "def gradientDescentBacktracking(f,gradf,x0,tol,alpha,beta):\n",
    "    \n",
    "    # set initial x value\n",
    "    x = x0\n",
    "    \n",
    "    # set tolerance so we don't skip the while loop\n",
    "    mag = tol + 1\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    # while loop\n",
    "    while mag > tol:\n",
    "        \n",
    "        # evaluate gradient\n",
    "        currentGrad = gradf(x)\n",
    "\n",
    "        # choose a descent direction (ex: the negative gradient)\n",
    "        deltax = -currentGrad\n",
    "\n",
    "        # choose a stepsize (backtracking search method)\n",
    "        stepsize = 1\n",
    "        \n",
    "        while f(x+stepsize*deltax) > f(x) + alpha * stepsize * deltax @ deltax:\n",
    "            stepsize = stepsize*beta\n",
    "\n",
    "        x = x + stepsize*deltax\n",
    "        \n",
    "        # check stopping criterion\n",
    "        mag = currentGrad @ currentGrad\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "    return [x,f(x),counter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2.00000432, 1.01078037]), 1.3524954942932903e-08, 1004]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientDescentBacktracking(f,gradf,x0,1e-10,0.4,0.49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2.00000353, 1.01208131]), 2.1316158905303505e-08, 854]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientDescentExact(f,gradf,x0,1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
